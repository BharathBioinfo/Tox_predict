{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference with FuncX\n",
    "Large-scale inference with FuncX on Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [07:38:45] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from funcx.sdk.client import FuncXClient\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "output_file = 'ena+db_tox21_screening.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the FuncX Client\n",
    "This is what we'll be using to connect to Theta for sending/recieving tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 0549fe6d-7eae-4949-aee6-468658f2ce93\n"
     ]
    }
   ],
   "source": [
    "fxc = FuncXClient()\n",
    "theta_ep = 'd3a23590-3282-429a-8bce-e0ca0f4177f3'\n",
    "with open('func_uuid.json') as fp:\n",
    "    func_id = json.load(fp)\n",
    "print(f'Running inference on {func_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Inference Requests\n",
    "Send out inferenece requests for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the data and make sure the SMILES are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 310782 molecules\n"
     ]
    }
   ],
   "source": [
    "database = pd.read_csv(os.path.join('..', '..', 'databases', 'ena+db.can'), header=None, delim_whitespace=True)\n",
    "print(f'Loaded {len(database)} molecules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.rename(columns={0: 'smiles'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310650 unique SMILES\n"
     ]
    }
   ],
   "source": [
    "database.drop_duplicates('smiles', inplace=True)\n",
    "print(f'Found {len(database)} unique SMILES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [07:38:47] Explicit valence for atom # 3 O, 3, is greater than permitted\n",
      "RDKit ERROR: [07:38:47] Explicit valence for atom # 12 Cl, 5, is greater than permitted\n",
      "RDKit ERROR: [07:38:47] Explicit valence for atom # 19 N, 5, is greater than permitted\n",
      "RDKit ERROR: [07:38:47] Explicit valence for atom # 16 Ga, 6, is greater than permitted\n",
      "RDKit ERROR: [07:38:47] Can't kekulize mol.  Unkekulized atoms: 2 3 5 7 10 12 13 19 21 22 28 29\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [07:38:47] Explicit valence for atom # 0 O, 3, is greater than permitted\n",
      "RDKit ERROR: [07:38:47] Explicit valence for atom # 3 Be, 4, is greater than permitted\n",
      "RDKit ERROR: [07:38:47] Can't kekulize mol.  Unkekulized atoms: 18 19 21\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [07:38:48] Explicit valence for atom # 20 Be, 3, is greater than permitted\n",
      "RDKit ERROR: [07:38:48] Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "RDKit ERROR: [07:38:48] Explicit valence for atom # 1 Cl, 4, is greater than permitted\n",
      "RDKit ERROR: [07:38:48] Explicit valence for atom # 2 B, 4, is greater than permitted\n",
      "RDKit ERROR: [07:38:49] Explicit valence for atom # 6 K, 2, is greater than permitted\n",
      "RDKit ERROR: [07:38:49] Explicit valence for atom # 12 Mg, 4, is greater than permitted\n",
      "RDKit ERROR: [07:38:49] Explicit valence for atom # 3 Ga, 6, is greater than permitted\n"
     ]
    }
   ],
   "source": [
    "database['invalid'] = database['smiles'].apply(lambda x: Chem.MolFromSmiles(x) is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310635 valid SMILES\n"
     ]
    }
   ],
   "source": [
    "database.query('not invalid', inplace=True)\n",
    "print(f'Found {len(database)} valid SMILES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxc.max_requests = 5000  # Enable faster task submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:25<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted 75 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "db_tasks = []\n",
    "for chunk in tqdm(np.array_split(database['smiles'], len(database) // batch_size)):\n",
    "    db_tasks.append(fxc.run(chunk.tolist(), endpoint_id=theta_ep, function_id=func_id))\n",
    "    sleep(0.1)\n",
    "print(f'Submitted {len(db_tasks)} tasks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "As results are returned, save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(status, path): \n",
    "    # Loop over all results in the status message\n",
    "    for key, result in status.items():\n",
    "        result = result['result']\n",
    "        # Parse the data\n",
    "        data = pd.DataFrame(result['result'])\n",
    "        exists = os.path.isfile(path)\n",
    "        \n",
    "        # Get the runtime and save it\n",
    "        data['task_id'] = key\n",
    "        data['runtime'] = (datetime.fromisoformat(result['end']) - datetime.fromisoformat(result['start'])).total_seconds()\n",
    "        data['start_time'] = result['start']\n",
    "        data['end_time'] = result['end']\n",
    "        data['host'] = result['hostname']\n",
    "        data['core_count'] = result['core_count']\n",
    "        \n",
    "        # Save the result to disk\n",
    "        data.to_csv(path, mode='a', header=not exists, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(output_file):\n",
    "    print(f'Removing old file: {output_file}')\n",
    "    os.unlink(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [08:22<00:00,  4.52s/it]"
     ]
    }
   ],
   "source": [
    "remaining_results = set(db_tasks)\n",
    "pbar = tqdm(total=len(db_tasks))\n",
    "while len(remaining_results) > 0:\n",
    "    # Get the status of the current tasks\n",
    "    status = fxc.get_batch_status(list(remaining_results))\n",
    "    \n",
    "    # Write the results to disk\n",
    "    write_results(status, output_file)\n",
    "    \n",
    "    # Update the list of results that are remaining\n",
    "    remaining_results.difference_update(status.keys())\n",
    "    pbar.update(len(status))\n",
    "    sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
